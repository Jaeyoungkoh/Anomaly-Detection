{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7234655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from folderconstants import *\n",
    "from shutil import copyfile\n",
    "\n",
    "def normalize(a):\n",
    "\ta = a / np.maximum(np.absolute(a.max(axis=0)), np.absolute(a.min(axis=0)))\n",
    "\treturn (a / 2 + 0.5)\n",
    "\n",
    "def normalize2(a, min_a = None, max_a = None):\n",
    "\tif min_a is None: min_a, max_a = min(a), max(a)\n",
    "\treturn (a - min_a) / (max_a - min_a), min_a, max_a\n",
    "\n",
    "def normalize2_1(a, min_a = None, max_a = None):\n",
    "\tif min_a is None: \n",
    "\t\tmin_a, max_a = np.min(a, axis=0), np.max(a, axis=0)\n",
    "\treturn (a - min_a) / (max_a - min_a), min_a, max_a\n",
    "\n",
    "def normalize3(a, min_a = None, max_a = None):\n",
    "\tif min_a is None: min_a, max_a = np.min(a, axis = 0), np.max(a, axis = 0)\n",
    "\treturn (a - min_a) / (max_a - min_a + 0.0001), min_a, max_a\n",
    "\n",
    "def convertNumpy(df):\n",
    "\tx = df[df.columns[3:]].values[::10, :]\n",
    "\treturn (x - x.min(0)) / (x.ptp(0) + 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acab9073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_save(category, filename, dataset, dataset_folder):\n",
    "    temp = np.genfromtxt(os.path.join(dataset_folder, category, filename),\n",
    "                         dtype=np.float64,\n",
    "                         delimiter=',')\n",
    "    print(dataset, category, filename, temp.shape)\n",
    "    np.save(os.path.join(output_folder, f\"SMD/{dataset}_{category}.npy\"), temp)\n",
    "    return temp.shape\n",
    "\n",
    "def load_and_save2(category, filename, dataset, dataset_folder, shape):\n",
    "\ttemp = np.zeros(shape)\n",
    "\twith open(os.path.join(dataset_folder, 'interpretation_label', filename), \"r\") as f:\n",
    "\t\tls = f.readlines()\n",
    "\tfor line in ls:\n",
    "\t\tpos, values = line.split(':')[0], line.split(':')[1].split(',')\n",
    "\t\tstart, end, indx = int(pos.split('-')[0]), int(pos.split('-')[1]), [int(i)-1 for i in values]\n",
    "\t\ttemp[start-1:end-1, indx] = 1\n",
    "\tprint(dataset, category, filename, temp.shape)\n",
    "\tnp.save(os.path.join(output_folder, f\"SMD/{dataset}_{category}.npy\"), temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30086f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset):\n",
    "\n",
    "\tfolder = os.path.join(output_folder, dataset) # /processed/SMD\n",
    "\tos.makedirs(folder, exist_ok=True)\n",
    "\n",
    "\tif dataset == 'SMD':\n",
    "\t\tdataset_folder = 'data/SMD'\n",
    "\t\tfile_list = os.listdir(os.path.join(dataset_folder, \"train\"))\n",
    "\t\tfor filename in file_list:\n",
    "\t\t\tif filename.endswith('.txt'):\n",
    "\t\t\t\tload_and_save('train', filename, filename.strip('.txt'), dataset_folder)\n",
    "\t\t\t\ts = load_and_save('test', filename, filename.strip('.txt'), dataset_folder)\n",
    "\t\t\t\tload_and_save2('labels', filename, filename.strip('.txt'), dataset_folder, s)\n",
    "\n",
    "\telif dataset in ['SMAP', 'MSL']:\n",
    "\t\tdataset_folder = 'data/SMAP_MSL'\n",
    "\t\tfile = os.path.join(dataset_folder, 'labeled_anomalies.csv')\n",
    "\t\tvalues = pd.read_csv(file)\n",
    "\t\tvalues = values[values['spacecraft'] == dataset]\n",
    "\t\tfilenames = values['chan_id'].values.tolist()\n",
    "\t\tfor fn in filenames:\n",
    "\t\t\ttrain = np.load(f'{dataset_folder}/train/{fn}.npy')\n",
    "\t\t\ttest = np.load(f'{dataset_folder}/test/{fn}.npy')\n",
    "\t\t\ttrain, min_a, max_a = normalize3(train)\n",
    "\t\t\ttest, _, _ = normalize3(test, min_a, max_a)\n",
    "\t\t\tnp.save(f'{folder}/{fn}_train.npy', train)\n",
    "\t\t\tnp.save(f'{folder}/{fn}_test.npy', test)\n",
    "\t\t\tlabels = np.zeros(test.shape)\n",
    "\t\t\tindices = values[values['chan_id'] == fn]['anomaly_sequences'].values[0]\n",
    "\t\t\tindices = indices.replace(']', '').replace('[', '').split(', ')\n",
    "\t\t\tindices = [int(i) for i in indices]\n",
    "\t\t\tfor i in range(0, len(indices), 2):\n",
    "\t\t\t\tlabels[indices[i]:indices[i+1], :] = 1\n",
    "\t\t\tnp.save(f'{folder}/{fn}_labels.npy', labels)\t\t\n",
    "\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fd65b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'MSL'\n",
    "load_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af7df28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train 데이터 shape: (28479, 38)\n",
      "         0         1         2         3    4         5         6    7   \\\n",
      "0  0.032258  0.039195  0.027871  0.024390  0.0  0.915385  0.343691  0.0   \n",
      "1  0.043011  0.048729  0.033445  0.025552  0.0  0.915385  0.344633  0.0   \n",
      "2  0.043011  0.034958  0.032330  0.025552  0.0  0.915385  0.344633  0.0   \n",
      "3  0.032258  0.028602  0.030100  0.024390  0.0  0.912821  0.342750  0.0   \n",
      "4  0.032258  0.019068  0.026756  0.023229  0.0  0.912821  0.342750  0.0   \n",
      "\n",
      "         8         9   ...   28        29        30        31   32        33  \\\n",
      "0  0.020011  0.000122  ...  0.0  0.004298  0.029993  0.022131  0.0  0.000045   \n",
      "1  0.019160  0.001722  ...  0.0  0.004298  0.030041  0.028821  0.0  0.000045   \n",
      "2  0.020011  0.000122  ...  0.0  0.004298  0.026248  0.021101  0.0  0.000045   \n",
      "3  0.021289  0.000000  ...  0.0  0.004298  0.030169  0.025733  0.0  0.000022   \n",
      "4  0.018734  0.000000  ...  0.0  0.004298  0.027240  0.022645  0.0  0.000034   \n",
      "\n",
      "         34        35   36   37  \n",
      "0  0.034677  0.034747  0.0  0.0  \n",
      "1  0.035763  0.035833  0.0  0.0  \n",
      "2  0.033012  0.033082  0.0  0.0  \n",
      "3  0.035112  0.035182  0.0  0.0  \n",
      "4  0.033447  0.033517  0.0  0.0  \n",
      "\n",
      "[5 rows x 38 columns]\n",
      " Test 데이터 shape: (28479, 38)\n",
      "         0         1         2         3    4         5         6    7   \\\n",
      "0  0.075269  0.065678  0.070234  0.074332  0.0  0.933333  0.274011  0.0   \n",
      "1  0.086022  0.080508  0.075808  0.076655  0.0  0.930769  0.274953  0.0   \n",
      "2  0.075269  0.064619  0.071349  0.074332  0.0  0.928205  0.274953  0.0   \n",
      "3  0.086022  0.048729  0.063545  0.070848  0.0  0.928205  0.273070  0.0   \n",
      "4  0.086022  0.051907  0.062430  0.070848  0.0  0.933333  0.274011  0.0   \n",
      "\n",
      "         8         9   ...   28        29        30        31        32  \\\n",
      "0  0.031081  0.000000  ...  0.0  0.008596  0.068036  0.048893  0.000386   \n",
      "1  0.031081  0.000122  ...  0.0  0.008596  0.070020  0.050437  0.000386   \n",
      "2  0.030940  0.000366  ...  0.0  0.008596  0.069684  0.055069  0.000386   \n",
      "3  0.027250  0.000244  ...  0.0  0.010029  0.073253  0.051467  0.000000   \n",
      "4  0.030940  0.000244  ...  0.0  0.008596  0.070932  0.051467  0.000386   \n",
      "\n",
      "         33        34        35   36   37  \n",
      "0  0.000034  0.064432  0.064500  0.0  0.0  \n",
      "1  0.000022  0.065228  0.065224  0.0  0.0  \n",
      "2  0.000045  0.067111  0.067178  0.0  0.0  \n",
      "3  0.000034  0.066676  0.066744  0.0  0.0  \n",
      "4  0.000022  0.066604  0.066671  0.0  0.0  \n",
      "\n",
      "[5 rows x 38 columns]\n",
      " Labels 데이터 shape: (28479, 38)\n",
      "    0    1    2    3    4    5    6    7    8    9   ...   28   29   30   31  \\\n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "\n",
      "    32   33   34   35   36   37  \n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 38 columns]\n",
      "15848  0     1.0\n",
      "       8     1.0\n",
      "       9     1.0\n",
      "       11    1.0\n",
      "       12    1.0\n",
      "       13    1.0\n",
      "       14    1.0\n",
      "15849  0     1.0\n",
      "       8     1.0\n",
      "       9     1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# .npy 파일이 저장된 폴더 경로를 지정해주세요.\n",
    "dataset = 'SMD'\n",
    "output_folder = './processed' \n",
    "\n",
    "# 각 파일의 전체 경로 생성\n",
    "train_path = os.path.join(output_folder, dataset, 'machine-1-1_train.npy')\n",
    "test_path = os.path.join(output_folder, dataset, 'machine-1-1_test.npy')\n",
    "labels_path = os.path.join(output_folder, dataset, 'machine-1-1_labels.npy')\n",
    "\n",
    "# .npy 파일 불러오기\n",
    "train_data = np.load(train_path)\n",
    "test_data = np.load(test_path)\n",
    "labels_data = np.load(labels_path)\n",
    "\n",
    "# Pandas DataFrame으로 변환하여 구조 확인\n",
    "df_train = pd.DataFrame(train_data)\n",
    "df_test = pd.DataFrame(test_data)\n",
    "df_labels = pd.DataFrame(labels_data)\n",
    "\n",
    "# 각 데이터의 shape과 상위 5개 행 출력\n",
    "print(f\" Train 데이터 shape: {df_train.shape}\")\n",
    "print(df_train.head())\n",
    "\n",
    "print(f\" Test 데이터 shape: {df_test.shape}\")\n",
    "print(df_test.head())\n",
    "\n",
    "print(f\" Labels 데이터 shape: {df_labels.shape}\")\n",
    "print(df_labels.head())\n",
    "\n",
    "# Labels 데이터에서 실제로 이상(1)으로 표시된 위치 찾아보기\n",
    "anomaly_points = df_labels[df_labels == 1].stack()\n",
    "\n",
    "print(anomaly_points.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71ede638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dmqa1905\\.conda\\envs\\dmqa\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1IYPnLUjaHMzwefzmxErQpb9UYGW_1dpQ\n",
      "From (redirected): https://drive.google.com/uc?id=1IYPnLUjaHMzwefzmxErQpb9UYGW_1dpQ&confirm=t&uuid=fcf45a6b-55fa-41f6-882f-36fb2201c584\n",
      "To: d:\\EUV_Anomaly_Detection\\SWaT_Dataset_Normal_v1.pkl\n",
      "100%|██████████| 217M/217M [00:07<00:00, 27.5MB/s] \n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=17TUUeUgCgEmPoR372yvg41iuOQzAKtUU\n",
      "From (redirected): https://drive.google.com/uc?id=17TUUeUgCgEmPoR372yvg41iuOQzAKtUU&confirm=t&uuid=8bf55467-5de0-42cb-8e13-b515bdba0ce7\n",
      "To: d:\\EUV_Anomaly_Detection\\SWaT_Dataset_Attack_v0.pkl\n",
      "100%|██████████| 197M/197M [00:06<00:00, 31.0MB/s] \n"
     ]
    }
   ],
   "source": [
    "# !pip install gdown\n",
    "import gdown\n",
    "\n",
    "train_url = 'https://drive.google.com/uc?id=1IYPnLUjaHMzwefzmxErQpb9UYGW_1dpQ'\n",
    "train_file = './SWaT_Dataset_Normal_v1.pkl'  \n",
    "gdown.download(train_url, train_file, quiet=False)\n",
    "\n",
    "test_url = 'https://drive.google.com/uc?id=17TUUeUgCgEmPoR372yvg41iuOQzAKtUU'\n",
    "test_file = './SWaT_Dataset_Attack_v0.pkl'  \n",
    "gdown.download(test_url, test_file, quiet=False)\n",
    "\n",
    "# 파일 경로 설정\n",
    "train_path = './data/SWaT/SWaT.A1 _ A2_Dec 2015/Physical/SWaT_Dataset_Normal_v1.pkl'\n",
    "test_path = './data/SWaT/SWaT.A1 _ A2_Dec 2015/Physical/SWaT_Dataset_Attack_v0.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d10ba13",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/SWaT/SWaT.A1 _ A2_Dec 2015/Physical/SWaT_Dataset_Normal_v1.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m valid_split_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.8\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 훈련 데이터셋 로드 및 전처리\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m trainset \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNormal/Attack\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Timestamp\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m valid_split_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(trainset) \u001b[38;5;241m*\u001b[39m valid_split_rate)\n\u001b[0;32m      7\u001b[0m validset \u001b[38;5;241m=\u001b[39m trainset\u001b[38;5;241m.\u001b[39miloc[valid_split_index:]\u001b[38;5;241m.\u001b[39mto_numpy()  \u001b[38;5;66;03m# 검증 데이터셋\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dmqa1905\\.conda\\envs\\dmqa\\lib\\site-packages\\pandas\\io\\pickle.py:185\u001b[0m, in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    184\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[1;32m--> 185\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\dmqa1905\\.conda\\envs\\dmqa\\lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/SWaT/SWaT.A1 _ A2_Dec 2015/Physical/SWaT_Dataset_Normal_v1.pkl'"
     ]
    }
   ],
   "source": [
    "# 검증 데이터 분할 비율\n",
    "valid_split_rate = 0.8\n",
    "\n",
    "# 훈련 데이터셋 로드 및 전처리\n",
    "trainset = pd.read_pickle(train_path).drop(['Normal/Attack', ' Timestamp'], axis=1)\n",
    "valid_split_index = int(len(trainset) * valid_split_rate)\n",
    "validset = trainset.iloc[valid_split_index:].to_numpy()  # 검증 데이터셋\n",
    "trainset = trainset.iloc[:valid_split_index].to_numpy()  # 훈련 데이터셋\n",
    "train_timestamp = np.arange(len(trainset))  # 훈련 데이터 타임스탬프\n",
    "valid_timestamp = np.arange(len(validset))  # 검증 데이터 타임스탬프\n",
    "\n",
    "# 테스트 데이터셋 로드 및 전처리\n",
    "testset = pd.read_pickle(test_path)\n",
    "test_timestamp = np.arange(len(testset))  # 테스트 데이터 타임스탬프\n",
    "test_label = testset['Normal/Attack'].copy()\n",
    "test_label[test_label == 'Normal'] = 0\n",
    "test_label[test_label != 0] = 1\n",
    "testset = testset.drop(['Normal/Attack', ' Timestamp'], axis=1)\n",
    "columns = testset.columns.tolist()  # 컬럼 리스트\n",
    "testset = testset.to_numpy()\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"Train set size: {len(trainset)}, Validation set size: {len(validset)}, Test set size: {len(testset)}\")\n",
    "print(f\"Columns: {columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1904cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
